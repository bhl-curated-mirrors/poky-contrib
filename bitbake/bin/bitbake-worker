#!/usr/bin/env python3
#
# SPDX-License-Identifier: GPL-2.0-only
#

import os
import sys
import warnings
warnings.simplefilter("default")
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(sys.argv[0])), 'lib'))
from bb import fetch2
import logging
import bb
import errno
import signal
import pickle
import traceback
import shlex
import subprocess
from multiprocessing import Lock
import asyncio

if sys.getfilesystemencoding() != "utf-8":
    sys.exit("Please use a locale setting which supports UTF-8 (such as LANG=en_US.UTF-8).\nPython can't change the filesystem locale after loading so we need a UTF-8 when Python starts or things won't work.")

# Users shouldn't be running this code directly
if len(sys.argv) != 2 or not sys.argv[1].startswith("decafbad"):
    print("bitbake-worker is meant for internal execution by bitbake itself, please don't use it standalone.")
    sys.exit(1)

profiling = False
if sys.argv[1].startswith("decafbadbad"):
    profiling = True
    try:
        import cProfile as profile
    except:
        import profile

# Unbuffer stdout to avoid log truncation in the event
# of an unorderly exit as well as to provide timely
# updates to log files for use with tail
try:
    if sys.stdout.name == '<stdout>':
        import fcntl
        fl = fcntl.fcntl(sys.stdout.fileno(), fcntl.F_GETFL)
        fl |= os.O_SYNC 
        fcntl.fcntl(sys.stdout.fileno(), fcntl.F_SETFL, fl)
        #sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)
except:
    pass

logger = logging.getLogger("BitBake")


async def connect_stdout():
    loop = asyncio.get_event_loop()
    w_transport, w_protocol = await loop.connect_write_pipe(asyncio.streams.FlowControlMixin, sys.stdout)
    writer = asyncio.StreamWriter(w_transport, w_protocol, None, loop)
    return writer

log_handler = bb.event.LogHandler()
logger.addHandler(log_handler)

if 0:
    # Code to write out a log file of all events passing through the worker
    logfilename = "/tmp/workerlogfile"
    format_str = "%(levelname)s: %(message)s"
    conlogformat = bb.msg.BBLogFormatter(format_str)
    consolelog = logging.FileHandler(logfilename)
    consolelog.setFormatter(conlogformat)
    logger.addHandler(consolelog)

async def read_messages(fd, handlers):
    buf = b""
    event = asyncio.Event()
    done = False

    def read_data():
        nonlocal buf
        nonlocal fd
        nonlocal event
        nonlocal done

        try:
            data = os.read(fd, 102400)
        except (OSError, IOError) as e:
            if e.errno != errno.EAGAIN:
                raise
            return

        if len(data) == 0:
            done = True
        else:
            buf += data
        event.set()

    asyncio.get_event_loop().add_reader(fd, read_data)

    try:
        while not done:
            for name, handler in handlers.items():
                prefix = b"<" + name + b">"
                if buf.startswith(prefix):
                    suffix = b"</" + name + b">"
                    index = buf.find(suffix)
                    if index != -1:
                        try:
                            workerlog_write("%d: Handling %r\n" % (fd, name))
                            await handler(buf[len(prefix):index])
                        except pickle.UnpicklingError:
                            workerlog_write("Unable to unpickle data: %s\n" % ":".join("{:02x}".format(c) for c in buf))
                            raise

                        buf = buf[index + len(suffix):]
                        break
                        # TODO: The old code would keep looking for an ending
                        # tag in a loop, so that a stream like
                        # <A>foo</A>bar</A> was valid. This doesn't appear to
                        # be necessary anymore?
                        #index = self.buf.find(b"</" + item + b">")
            else:
                # Nothing found in the buffer. Wait for more data
                await event.wait()
                event.clear()
    finally:
        asyncio.get_event_loop().remove_reader(fd)

class ChildHandler(object):
    def __init__(self, writer, task, pid, pipeinfd, pipeoutfd):
        self.task = task
        self.writer = writer
        self.pid = pid
        self.pipeinfd = pipeinfd
        if pipeoutfd >= 0:
            os.close(pipeoutfd)

        self.done_event = asyncio.Event()
        self.loop = asyncio.get_running_loop()

        asyncio.get_child_watcher().add_child_handler(self.pid, self._child_watcher_callback)

    def _child_watcher_callback(self, pid, status):
        # The callback may be called in a thread, so call_soon_threadsafe is
        # recommended to get back to the main loop.
        self.loop.call_soon_threadsafe(self.child_exited, pid, status)

    async def main_loop(self):
        bb.utils.nonblockingfd(self.pipeinfd)
        await read_messages(self.pipeinfd, {
            b"event": self.handle_event,
        })
        os.close(self.pipeinfd)
        await self.done_event.wait()

    async def handle_event(self, data):
        self.writer.write(b"<event>")
        self.writer.write(data)
        self.writer.write(b"</event>")
        await self.writer.drain()

    def child_exited(self, pid, status):
        try:
            if pid != self.pid:
                return

            workerlog_write("Exit code of %d for pid %d (fd %d)\n" % (status, pid, self.pipeinfd))

            asyncio.get_child_watcher().remove_child_handler(self.pid)

            if os.WIFEXITED(status):
                status = os.WEXITSTATUS(status)
            elif os.WIFSIGNALED(status):
                # Per shell conventions for $?, when a process exits due to
                # a signal, we return an exit code of 128 + SIGNUM
                status = 128 + os.WTERMSIG(status)

            self.writer.write(b"<exitcode>")
            self.writer.write(pickle.dumps((self.task, status)))
            self.writer.write(b"</exitcode>")

            self.done_event.set()
        except Exception as e:
            workerlog_write("%s\n%s\n" % (traceback.format_exc(), e))
            raise e

    def close(self):
        if not self.done_event.is_set():
            try:
                os.kill(-self.pid, signal.SIGTERM)
            except OSError:
                pass


class MainHandler(object):
    def __init__(self, writer):
        self.writer = writer
        self.cookercfg = None
        self.databuilder = None
        self.data = None
        self.extraconfigdata = None
        self.children = []
        self.child_tasks = []

    async def main_loop(self):
        loop = asyncio.get_running_loop()
        fd = sys.stdin.fileno()
        bb.utils.nonblockingfd(fd)

        loop.add_signal_handler(signal.SIGTERM, self.signal_handler)
        loop.add_signal_handler(signal.SIGHUP, self.signal_handler)

        try:
            await read_messages(fd, {
                b"cookerconfig": self.handle_cookercfg,
                b"extraconfigdata": self.handle_extraconfigdata,
                b"workerdata": self.handle_workerdata,
                b"newtaskhashes": self.handle_newtaskhashes,
                b"runtask": self.handle_runtask,
                b"finishnow": self.handle_finishnow,
                b"ping": self.handle_ping,
                b"quit": self.handle_quit,
                }
            )
        finally:
            loop.remove_signal_handler(signal.SIGTERM)
            loop.remove_signal_handler(signal.SIGHUP)

    def signal_handler(self, signum, stackframe):
        loop = asyncio.get_running_loop()

        if signum == signal.SIGTERM:
            bb.warn("Worker received SIGTERM, shutting down...")
        elif signum == signal.SIGHUP:
            bb.warn("Worker received SIGHUP, shutting down...")

        self.handle_finishnow(None)
        loop.remove_signal_handler(signal.SIGTERM)
        os.kill(os.getpid(), signal.SIGTERM)

    async def handle_cookercfg(self, data):
        self.cookercfg = pickle.loads(data)
        self.databuilder = bb.cookerdata.CookerDataBuilder(self.cookercfg, worker=True)
        self.databuilder.parseBaseConfiguration()
        self.data = self.databuilder.data

    async def handle_extraconfigdata(self, data):
        self.extraconfigdata = pickle.loads(data)

    async def handle_workerdata(self, data):
        self.workerdata = pickle.loads(data)
        bb.build.verboseShellLogging = self.workerdata["build_verbose_shell"]
        bb.build.verboseStdoutLogging = self.workerdata["build_verbose_stdout"]
        bb.msg.loggerDefaultLogLevel = self.workerdata["logdefaultlevel"]
        bb.msg.loggerDefaultDomains = self.workerdata["logdefaultdomain"]
        for mc in self.databuilder.mcdata:
            self.databuilder.mcdata[mc].setVar("PRSERV_HOST", self.workerdata["prhost"])
            self.databuilder.mcdata[mc].setVar("BB_HASHSERVE", self.workerdata["hashservaddr"])

    async def handle_newtaskhashes(self, data):
        self.workerdata["newhashes"] = pickle.loads(data)

    async def handle_ping(self, _):
        logger.warning("Pong from bitbake-worker!")

    async def handle_quit(self, data):
        global normalexit
        normalexit = True
        sys.exit(0)

    async def run_child(self, child):
        await child.main_loop()
        self.children.remove(child)
        self.child_tasks.remove(asyncio.current_task())

    async def handle_runtask(self, data):
        fn, task, taskname, taskhash, unihash, quieterrors, appends, taskdepdata, dry_run_exec = pickle.loads(data)
        workerlog_write("Handling runtask %s %s %s\n" % (task, fn, taskname))

        pid, pipeinfd, pipeoutfd = fork_off_task(self.cookercfg, self.data, self.databuilder, self.workerdata, fn, task, taskname, taskhash, unihash, appends, taskdepdata, self.extraconfigdata, quieterrors, dry_run_exec)

        child = ChildHandler(self.writer, task, pid, pipeinfd, pipeoutfd)
        self.children.append(child)

        t = asyncio.ensure_future(self.run_child(child))
        self.child_tasks.append(t)

    async def handle_finishnow(self, _=None):
        for c in self.children:
            c.close()

        workerlog_write("Waiting for %d child tasks: %s\n" % (len(self.child_tasks),
            " ".join(str(c.pid) for c in self.children)))

        # Wait for all outstanding children to exit
        await asyncio.gather(*self.child_tasks)

async def main():
    writer = await connect_stdout()
    worker_queue = []

    def worker_fire(event, d):
        nonlocal worker_queue

        async def flush_worker_queue():
            nonlocal writer
            nonlocal worker_queue

            if worker_queue:
                for m in worker_queue:
                    writer.write(m)
                worker_queue = []
                await writer.drain()

        # To ensure the messages are sent out in the order they are received,
        # put them in a list then schedule a task to write them out
        data = b"<event>" + pickle.dumps(event) + b"</event>"
        worker_queue.append(data)
        asyncio.ensure_future(flush_worker_queue())

    bb.event.worker_fire = worker_fire

    handler = MainHandler(writer)

    await asyncio.gather(handler.main_loop())

normalexit = False

lf = None
#lf = open("/tmp/workercommandlog", "w+")
def workerlog_write(msg):
    global lf
    if lf:
        lf.write(msg)
        lf.flush()


def sigterm_handler(signum, frame):
    signal.signal(signal.SIGTERM, signal.SIG_DFL)
    os.killpg(0, signal.SIGTERM)
    sys.exit()

def fork_off_task(cfg, data, databuilder, workerdata, fn, task, taskname, taskhash, unihash, appends, taskdepdata, extraconfigdata, quieterrors=False, dry_run_exec=False):
    # We need to setup the environment BEFORE the fork, since
    # a fork() or exec*() activates PSEUDO...

    envbackup = {}
    fakeroot = False
    fakeenv = {}
    umask = None

    uid = os.getuid()
    gid = os.getgid()


    taskdep = workerdata["taskdeps"][fn]
    if 'umask' in taskdep and taskname in taskdep['umask']:
        umask = taskdep['umask'][taskname]
    elif workerdata["umask"]:
        umask = workerdata["umask"]
    if umask:
        # umask might come in as a number or text string..
        try:
             umask = int(umask, 8)
        except TypeError:
             pass

    dry_run = cfg.dry_run or dry_run_exec

    # We can't use the fakeroot environment in a dry run as it possibly hasn't been built
    if 'fakeroot' in taskdep and taskname in taskdep['fakeroot'] and not dry_run:
        fakeroot = True
        envvars = (workerdata["fakerootenv"][fn] or "").split()
        for key, value in (var.split('=') for var in envvars):
            envbackup[key] = os.environ.get(key)
            os.environ[key] = value
            fakeenv[key] = value

        fakedirs = (workerdata["fakerootdirs"][fn] or "").split()
        for p in fakedirs:
            bb.utils.mkdirhier(p)
        logger.debug2('Running %s:%s under fakeroot, fakedirs: %s' %
                        (fn, taskname, ', '.join(fakedirs)))
    else:
        envvars = (workerdata["fakerootnoenv"][fn] or "").split()
        for key, value in (var.split('=') for var in envvars):
            envbackup[key] = os.environ.get(key)
            os.environ[key] = value
            fakeenv[key] = value

    sys.stdout.flush()
    sys.stderr.flush()

    try:
        pipeinfd, pipeoutfd = os.pipe()
        pid = os.fork()
    except OSError as e:
        logger.critical("fork failed: %d (%s)" % (e.errno, e.strerror))
        sys.exit(1)

    if pid == 0:
        def child():
            os.close(pipeinfd)

            bb.utils.signal_on_parent_exit("SIGTERM")

            pipeout = os.fdopen(pipeoutfd, 'wb', 0)
            pipelock = Lock()
            def worker_child_fire(event, d):
                nonlocal pipeout
                nonlocal pipelock

                data = b"<event>" + pickle.dumps(event) + b"</event>"
                try:
                    with pipelock:
                        while(len(data)):
                            written = pipeout.write(data)
                            data = data[written:]
                except IOError:
                    sigterm_handler(None, None)
                    raise

            # Save out the PID so that the event can include it the
            # events
            bb.event.worker_pid = os.getpid()
            bb.event.worker_fire = worker_child_fire

            # Make the child the process group leader and ensure no
            # child process will be controlled by the current terminal
            # This ensures signals sent to the controlling terminal like Ctrl+C
            # don't stop the child processes.
            os.setsid()

            signal.signal(signal.SIGTERM, sigterm_handler)
            # Let SIGHUP exit as SIGTERM
            signal.signal(signal.SIGHUP, sigterm_handler)

            # No stdin
            newsi = os.open(os.devnull, os.O_RDWR)
            os.dup2(newsi, sys.stdin.fileno())

            if umask:
                os.umask(umask)

            try:
                bb_cache = bb.cache.NoCache(databuilder)
                (realfn, virtual, mc) = bb.cache.virtualfn2realfn(fn)
                the_data = databuilder.mcdata[mc]
                the_data.setVar("BB_WORKERCONTEXT", "1")
                the_data.setVar("BB_TASKDEPDATA", taskdepdata)
                the_data.setVar('BB_CURRENTTASK', taskname.replace("do_", ""))
                if cfg.limited_deps:
                    the_data.setVar("BB_LIMITEDDEPS", "1")
                the_data.setVar("BUILDNAME", workerdata["buildname"])
                the_data.setVar("DATE", workerdata["date"])
                the_data.setVar("TIME", workerdata["time"])
                for varname, value in extraconfigdata.items():
                    the_data.setVar(varname, value)

                bb.parse.siggen.set_taskdata(workerdata["sigdata"])
                if "newhashes" in workerdata:
                    bb.parse.siggen.set_taskhashes(workerdata["newhashes"])
                ret = 0

                the_data = bb_cache.loadDataFull(fn, appends)
                the_data.setVar('BB_TASKHASH', taskhash)
                the_data.setVar('BB_UNIHASH', unihash)

                bb.utils.set_process_name("%s:%s" % (the_data.getVar("PN"), taskname.replace("do_", "")))

                if not the_data.getVarFlag(taskname, 'network', False):
                    logger.debug("Attempting to disable network")
                    bb.utils.disable_network(uid, gid)

                # exported_vars() returns a generator which *cannot* be passed to os.environ.update() 
                # successfully. We also need to unset anything from the environment which shouldn't be there 
                exports = bb.data.exported_vars(the_data)

                bb.utils.empty_environment()
                for e, v in exports:
                    os.environ[e] = v

                for e in fakeenv:
                    os.environ[e] = fakeenv[e]
                    the_data.setVar(e, fakeenv[e])
                    the_data.setVarFlag(e, 'export', "1")

                task_exports = the_data.getVarFlag(taskname, 'exports')
                if task_exports:
                    for e in task_exports.split():
                        the_data.setVarFlag(e, 'export', '1')
                        v = the_data.getVar(e)
                        if v is not None:
                            os.environ[e] = v

                if quieterrors:
                    the_data.setVarFlag(taskname, "quieterrors", "1")

            except Exception:
                if not quieterrors:
                    logger.critical(traceback.format_exc())
                os._exit(1)
            try:
                if dry_run:
                    return 0
                try:
                    ret = bb.build.exec_task(fn, taskname, the_data, cfg.profile)
                finally:
                    if fakeroot:
                        fakerootcmd = shlex.split(the_data.getVar("FAKEROOTCMD"))
                        subprocess.run(fakerootcmd + ['-S'], check=True, stdout=subprocess.PIPE)
                return ret
            except:
                os._exit(1)
        if not profiling:
            os._exit(child())
        else:
            profname = "profile-%s.log" % (fn.replace("/", "-") + "-" + taskname)
            prof = profile.Profile()
            try: 
                ret = profile.Profile.runcall(prof, child)
            finally:
                prof.dump_stats(profname)
                bb.utils.process_profilelog(profname)
                os._exit(ret)
    else:
        for key, value in iter(envbackup.items()):
            if value is None:
                del os.environ[key]
            else:
                os.environ[key] = value

    return pid, pipeinfd, pipeoutfd

try:
    if "beef" in sys.argv[1]:
        bb.utils.set_process_name("Worker (Fakeroot)")
    else:
        bb.utils.set_process_name("Worker")

    loop = asyncio.get_event_loop()

    if not profiling:
        loop.run_until_complete(main())
    else:
        profname = "profile-worker.log"
        prof = profile.Profile()
        try:
            profile.Profile.runcall(prof, loop.run_until_complete, main())
        finally:
            prof.dump_stats(profname)
            bb.utils.process_profilelog(profname)
except Exception as e:
    workerlog_write("%s\n%s\n" % (traceback.format_exc(), e))
    if not normalexit:
        sys.stderr.write(traceback.format_exc())
        sys.stderr.write(str(e))

workerlog_write("exiting\n")
if not normalexit:
    sys.exit(1)
sys.exit(0)
